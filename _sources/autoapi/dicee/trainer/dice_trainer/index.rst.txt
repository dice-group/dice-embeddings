dicee.trainer.dice_trainer
==========================

.. py:module:: dicee.trainer.dice_trainer

.. autoapi-nested-parse::

   DICE Trainer module for knowledge graph embedding training.

   Provides the DICE_Trainer class which supports multiple training backends
   including PyTorch Lightning, DDP, and custom CPU/GPU trainers.



Classes
-------

.. autoapisummary::

   dicee.trainer.dice_trainer.DICE_Trainer


Functions
---------

.. autoapisummary::

   dicee.trainer.dice_trainer.load_term_mapping
   dicee.trainer.dice_trainer.initialize_trainer
   dicee.trainer.dice_trainer.get_callbacks


Module Contents
---------------

.. py:function:: load_term_mapping(file_path: str) -> polars.DataFrame

   Load term-to-index mapping from CSV file.

   :param file_path: Base path without extension.

   :returns: Polars DataFrame containing the mapping.


.. py:function:: initialize_trainer(args, callbacks: List) -> Union[dicee.trainer.torch_trainer.TorchTrainer, dicee.trainer.torch_trainer_ddp.TorchDDPTrainer, lightning.Trainer]

   Initialize the appropriate trainer based on configuration.

   :param args: Configuration arguments containing trainer type.
   :param callbacks: List of training callbacks.

   :returns: Initialized trainer instance.

   :raises AssertionError: If trainer is None after initialization.


.. py:function:: get_callbacks(args) -> List

   Create list of callbacks based on configuration.

   :param args: Configuration arguments.

   :returns: List of callback instances.


.. py:class:: DICE_Trainer(args, is_continual_training: bool, storage_path, evaluator=None)

   DICE_Trainer implement
    1- Pytorch Lightning trainer (https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html)
    2- Multi-GPU Trainer(https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
    3- CPU Trainer

    Parameter
    ---------
    args

    is_continual_training:bool

    storage_path:str

    evaluator:

    Returns
    -------
    report:dict



   .. py:attribute:: report


   .. py:attribute:: args


   .. py:attribute:: trainer
      :value: None



   .. py:attribute:: is_continual_training


   .. py:attribute:: storage_path


   .. py:attribute:: evaluator
      :value: None



   .. py:attribute:: form_of_labelling
      :value: None



   .. py:method:: continual_start(knowledge_graph)

      (1) Initialize training.
      (2) Load model
      (3) Load trainer
      (3) Fit model

      Parameter
      ---------

      :returns: * *model*
                * **form_of_labelling** (*str*)



   .. py:method:: initialize_trainer(callbacks: List) -> lightning.Trainer | dicee.trainer.torch_trainer.TorchTrainer | dicee.trainer.torch_trainer_ddp.TorchDDPTrainer

      Initialize Trainer from input arguments



   .. py:method:: initialize_or_load_model()


   .. py:method:: init_dataloader(dataset: torch.utils.data.Dataset) -> torch.utils.data.DataLoader


   .. py:method:: init_dataset() -> torch.utils.data.Dataset


   .. py:method:: start(knowledge_graph: Union[dicee.knowledge_graph.KG, numpy.memmap]) -> Tuple[dicee.models.base_model.BaseKGE, str]

      Start the training

      (1) Initialize Trainer
      (2) Initialize or load a pretrained KGE model

      in DDP setup, we need to load the memory map of already read/index KG.



   .. py:method:: k_fold_cross_validation(dataset) -> Tuple[dicee.models.base_model.BaseKGE, str]

      Perform K-fold Cross-Validation

      1. Obtain K train and test splits.
      2. For each split,
          2.1 initialize trainer and model
          2.2. Train model with configuration provided in args.
          2.3. Compute the mean reciprocal rank (MRR) score of the model on the test respective split.
      3. Report the mean and average MRR .

      :param self:
      :param dataset:
      :return: model



