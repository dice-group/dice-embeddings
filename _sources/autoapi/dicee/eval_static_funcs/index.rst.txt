dicee.eval_static_funcs
=======================

.. py:module:: dicee.eval_static_funcs


Functions
---------

.. autoapisummary::

   dicee.eval_static_funcs.evaluate_link_prediction_performance
   dicee.eval_static_funcs.evaluate_link_prediction_performance_with_reciprocals
   dicee.eval_static_funcs.evaluate_link_prediction_performance_with_bpe_reciprocals
   dicee.eval_static_funcs.evaluate_link_prediction_performance_with_bpe
   dicee.eval_static_funcs.evaluate_lp_bpe_k_vs_all
   dicee.eval_static_funcs.evaluate_literal_prediction
   dicee.eval_static_funcs.evaluate_ensemble_link_prediction_performance


Module Contents
---------------

.. py:function:: evaluate_link_prediction_performance(model: dicee.knowledge_graph_embeddings.KGE, triples, er_vocab: Dict[Tuple, List], re_vocab: Dict[Tuple, List]) -> Dict

   :param model:
   :param triples:
   :param er_vocab:
   :param re_vocab:


.. py:function:: evaluate_link_prediction_performance_with_reciprocals(model: dicee.knowledge_graph_embeddings.KGE, triples, er_vocab: Dict[Tuple, List])

.. py:function:: evaluate_link_prediction_performance_with_bpe_reciprocals(model: dicee.knowledge_graph_embeddings.KGE, within_entities: List[str], triples: List[List[str]], er_vocab: Dict[Tuple, List])

.. py:function:: evaluate_link_prediction_performance_with_bpe(model: dicee.knowledge_graph_embeddings.KGE, within_entities: List[str], triples: List[Tuple[str]], er_vocab: Dict[Tuple, List], re_vocab: Dict[Tuple, List])

   :param model:
   :param triples:
   :param within_entities:
   :param er_vocab:
   :param re_vocab:


.. py:function:: evaluate_lp_bpe_k_vs_all(model, triples: List[List[str]], er_vocab=None, batch_size=None, func_triple_to_bpe_representation: Callable = None, str_to_bpe_entity_to_idx=None)

.. py:function:: evaluate_literal_prediction(kge_model: dicee.knowledge_graph_embeddings.KGE, eval_file_path: str = None, store_lit_preds: bool = True, eval_literals: bool = True, loader_backend: str = 'pandas', return_attr_error_metrics: bool = False)

   Evaluates the trained literal prediction model on a test file.

   :param eval_file_path: Path to the evaluation file.
   :type eval_file_path: str
   :param store_lit_preds: If True, stores the predictions in a CSV file.
   :type store_lit_preds: bool
   :param eval_literals: If True, evaluates the literal predictions and prints error metrics.
   :type eval_literals: bool
   :param loader_backend: Backend for loading the dataset ('pandas' or 'rdflib').
   :type loader_backend: str

   :returns: DataFrame containing error metrics for each attribute if return_attr_error_metrics is True.
   :rtype: pd.DataFrame

   :raises RuntimeError: If the kGE model does not have a trained literal model.
   :raises AssertionError: If the kGE model is not an instance of KGE or if the test set has no valid entities or attributes.


.. py:function:: evaluate_ensemble_link_prediction_performance(models, triples, er_vocab: Dict[Tuple, List], weights: List[float] = None, batch_size: int = 512, weighted_averaging: bool = True, normalize_scores: bool = True) -> Dict

   Evaluates link prediction performance of an ensemble of KGE models.
   :param models: List of KGE models (snapshots)
   :param triples: np.ndarray or list of lists, shape (N,3), all integer indices (head, rel, tail)
   :param er_vocab: Dict[Tuple, List]
                    Mapping (head_idx, rel_idx) â†’ list of tail_idx to filter (incl. target).
   :param weights: Optional[List[float]]
                   Weights for model averaging. If None, use uniform (=simple mean).
   :param batch_size: int

   :returns: dict of link prediction metrics (H@1, H@3, H@10, MRR)


