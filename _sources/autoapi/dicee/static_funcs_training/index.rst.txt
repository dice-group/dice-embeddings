dicee.static_funcs_training
===========================

.. py:module:: dicee.static_funcs_training

.. autoapi-nested-parse::

   Training-related static functions.

   This module provides backward compatibility by re-exporting evaluation
   functions from the new dicee.evaluation module, along with training utilities.

   .. deprecated::
       Evaluation functions have moved to ``dicee.evaluation``. Use that module
       for new code. This module will continue to export training utilities.



Functions
---------

.. autoapisummary::

   dicee.static_funcs_training.evaluate_lp
   dicee.static_funcs_training.evaluate_bpe_lp
   dicee.static_funcs_training.make_iterable_verbose
   dicee.static_funcs_training.efficient_zero_grad


Module Contents
---------------

.. py:function:: evaluate_lp(model, triple_idx, num_entities: int, er_vocab: Dict[Tuple, List], re_vocab: Dict[Tuple, List], info: str = 'Eval Starts', batch_size: int = 128, chunk_size: int = 1000) -> Dict[str, float]

   Evaluate link prediction with batched processing.

   Memory-efficient evaluation using chunked entity scoring.

   :param model: The KGE model to evaluate.
   :param triple_idx: Integer-indexed triples as numpy array.
   :param num_entities: Total number of entities.
   :param er_vocab: Mapping (head_idx, rel_idx) -> list of tail indices.
   :param re_vocab: Mapping (rel_idx, tail_idx) -> list of head indices.
   :param info: Description to print.
   :param batch_size: Batch size for triple processing.
   :param chunk_size: Chunk size for entity scoring.

   :returns: Dictionary with H@1, H@3, H@10, and MRR metrics.


.. py:function:: evaluate_bpe_lp(model, triple_idx: List[Tuple], all_bpe_shaped_entities, er_vocab: Dict[Tuple, List], re_vocab: Dict[Tuple, List], info: str = 'Eval Starts') -> Dict[str, float]

   Evaluate link prediction with BPE-encoded entities.

   :param model: The KGE model to evaluate.
   :param triple_idx: List of BPE-encoded triple tuples.
   :param all_bpe_shaped_entities: All entities with BPE representations.
   :param er_vocab: Mapping for tail filtering.
   :param re_vocab: Mapping for head filtering.
   :param info: Description to print.

   :returns: Dictionary with H@1, H@3, H@10, and MRR metrics.


.. py:function:: make_iterable_verbose(iterable_object: Iterable, verbose: bool, desc: str = 'Default', position: Optional[int] = None, leave: bool = True) -> Iterable

   Wrap an iterable with tqdm progress bar if verbose is True.

   :param iterable_object: The iterable to potentially wrap.
   :param verbose: Whether to show progress bar.
   :param desc: Description for the progress bar.
   :param position: Position of the progress bar.
   :param leave: Whether to leave the progress bar after completion.

   :returns: The original iterable or a tqdm-wrapped version.


.. py:function:: efficient_zero_grad(model) -> None

   Efficiently zero gradients using parameter.grad = None.

   This is more efficient than optimizer.zero_grad() as it avoids
   memory operations.

   See: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html

   :param model: PyTorch model to zero gradients for.


