dicee.models.real
=================

.. py:module:: dicee.models.real


Classes
-------

.. autoapisummary::

   dicee.models.real.DistMult
   dicee.models.real.TransE
   dicee.models.real.Shallom
   dicee.models.real.Pyke
   dicee.models.real.CoKEConfig
   dicee.models.real.CoKE


Module Contents
---------------

.. py:class:: DistMult(args)

   Bases: :py:obj:`dicee.models.base_model.BaseKGE`


   Embedding Entities and Relations for Learning and Inference in Knowledge Bases
   https://arxiv.org/abs/1412.6575


   .. py:attribute:: name
      :value: 'DistMult'



   .. py:method:: k_vs_all_score(emb_h: torch.FloatTensor, emb_r: torch.FloatTensor, emb_E: torch.FloatTensor)

      :param emb_h:
      :param emb_r:
      :param emb_E:



   .. py:method:: forward_k_vs_all(x: torch.LongTensor)


   .. py:method:: forward_k_vs_sample(x: torch.LongTensor, target_entity_idx: torch.LongTensor)


   .. py:method:: score(h, r, t)


.. py:class:: TransE(args)

   Bases: :py:obj:`dicee.models.base_model.BaseKGE`


   Translating Embeddings for Modeling
   Multi-relational Data
   https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf


   .. py:attribute:: name
      :value: 'TransE'



   .. py:attribute:: margin
      :value: 4



   .. py:method:: score(head_ent_emb, rel_ent_emb, tail_ent_emb)


   .. py:method:: forward_k_vs_all(x: torch.Tensor) -> torch.FloatTensor


.. py:class:: Shallom(args)

   Bases: :py:obj:`dicee.models.base_model.BaseKGE`


   A shallow neural model for relation prediction (https://arxiv.org/abs/2101.09090)


   .. py:attribute:: name
      :value: 'Shallom'



   .. py:attribute:: shallom


   .. py:method:: get_embeddings() -> Tuple[numpy.ndarray, None]


   .. py:method:: forward_k_vs_all(x) -> torch.FloatTensor


   .. py:method:: forward_triples(x) -> torch.FloatTensor

      :param x:
      :return:



.. py:class:: Pyke(args)

   Bases: :py:obj:`dicee.models.base_model.BaseKGE`


   A Physical Embedding Model for Knowledge Graphs


   .. py:attribute:: name
      :value: 'Pyke'



   .. py:attribute:: dist_func


   .. py:attribute:: margin
      :value: 1.0



   .. py:method:: forward_triples(x: torch.LongTensor)

      :param x:



.. py:class:: CoKEConfig

   Configuration for the CoKE (Contextualized Knowledge Graph Embedding) model.

   .. attribute:: block_size

      Sequence length for transformer (3 for triples: head, relation, tail)

   .. attribute:: vocab_size

      Total vocabulary size (num_entities + num_relations)

   .. attribute:: n_layer

      Number of transformer layers

   .. attribute:: n_head

      Number of attention heads per layer

   .. attribute:: n_embd

      Embedding dimension (set to match model embedding_dim)

   .. attribute:: dropout

      Dropout rate applied throughout the model

   .. attribute:: bias

      Whether to use bias in linear layers

   .. attribute:: causal

      Whether to use causal masking (False for bidirectional attention)


   .. py:attribute:: block_size
      :type:  int
      :value: 3



   .. py:attribute:: vocab_size
      :type:  int
      :value: None



   .. py:attribute:: n_layer
      :type:  int
      :value: 6



   .. py:attribute:: n_head
      :type:  int
      :value: 8



   .. py:attribute:: n_embd
      :type:  int
      :value: None



   .. py:attribute:: dropout
      :type:  float
      :value: 0.3



   .. py:attribute:: bias
      :type:  bool
      :value: True



   .. py:attribute:: causal
      :type:  bool
      :value: False



.. py:class:: CoKE(args, config: CoKEConfig = CoKEConfig())

   Bases: :py:obj:`dicee.models.base_model.BaseKGE`


   Contextualized Knowledge Graph Embedding (CoKE) model.
   Based on: https://arxiv.org/pdf/1911.02168.

   CoKE uses a transformer encoder to learn contextualized representations of entities and relations.
   For link prediction, it predicts masked elements in (head, relation, tail) triples using
   bidirectional attention, similar to BERT's masked language modeling approach.

   The model creates a sequence [head_emb, relation_emb, mask_emb], adds positional embeddings,
   and processes it through transformer layers to predict the tail entity.


   .. py:attribute:: name
      :value: 'CoKE'



   .. py:attribute:: config


   .. py:attribute:: pos_emb


   .. py:attribute:: mask_emb


   .. py:attribute:: blocks


   .. py:attribute:: ln_f


   .. py:attribute:: coke_dropout


   .. py:method:: forward_k_vs_all(x: torch.Tensor)


   .. py:method:: score(emb_h, emb_r, emb_t)


   .. py:method:: forward_k_vs_sample(x: torch.LongTensor, target_entity_idx: torch.LongTensor)


