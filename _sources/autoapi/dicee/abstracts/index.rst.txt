dicee.abstracts
===============

.. py:module:: dicee.abstracts


Classes
-------

.. autoapisummary::

   dicee.abstracts.AbstractTrainer
   dicee.abstracts.BaseInteractiveKGE
   dicee.abstracts.InteractiveQueryDecomposition
   dicee.abstracts.AbstractCallback
   dicee.abstracts.AbstractPPECallback
   dicee.abstracts.BaseInteractiveTrainKGE


Module Contents
---------------

.. py:class:: AbstractTrainer(args, callbacks)

   Abstract class for Trainer class for knowledge graph embedding models


   Parameter
   ---------
   args : str
       ?

   callbacks: list
           ?


   .. py:attribute:: attributes


   .. py:attribute:: callbacks


   .. py:attribute:: is_global_zero
      :value: True



   .. py:attribute:: global_rank
      :value: 0



   .. py:attribute:: local_rank
      :value: 0



   .. py:attribute:: strategy
      :value: None



   .. py:method:: on_fit_start(*args, **kwargs)

      A function to call callbacks before the training starts.

      Parameter
      ---------
      args

      kwargs


      :rtype: None



   .. py:method:: on_fit_end(*args, **kwargs)

      A function to call callbacks at the ned of the training.

      Parameter
      ---------
      args

      kwargs


      :rtype: None



   .. py:method:: on_train_epoch_end(*args, **kwargs)

      A function to call callbacks at the end of an epoch.

      Parameter
      ---------
      args

      kwargs


      :rtype: None



   .. py:method:: on_train_batch_end(*args, **kwargs)

      A function to call callbacks at the end of each mini-batch during training.

      Parameter
      ---------
      args

      kwargs


      :rtype: None



   .. py:method:: save_checkpoint(full_path: str, model) -> None
      :staticmethod:


      A static function to save a model into disk

      Parameter
      ---------
      full_path : str

      model:


      :rtype: None



.. py:class:: BaseInteractiveKGE(path: str = None, url: str = None, construct_ensemble: bool = False, model_name: str = None, apply_semantic_constraint: bool = False)

   Abstract/base class for using knowledge graph embedding models interactively.


   Parameter
   ---------
   path_of_pretrained_model_dir : str
       ?

   construct_ensemble: boolean
           ?

   model_name: str
   apply_semantic_constraint : boolean


   .. py:attribute:: construct_ensemble
      :value: False



   .. py:attribute:: apply_semantic_constraint
      :value: False



   .. py:attribute:: configs


   .. py:method:: get_eval_report() -> dict


   .. py:method:: get_bpe_token_representation(str_entity_or_relation: Union[List[str], str]) -> Union[List[List[int]], List[int]]

      :param str_entity_or_relation:
      :type str_entity_or_relation: corresponds to a str or a list of strings to be tokenized via BPE and shaped.

      :rtype: A list integer(s) or a list of lists containing integer(s)



   .. py:method:: get_padded_bpe_triple_representation(triples: List[List[str]]) -> Tuple[List, List, List]

      :param triples:



   .. py:method:: set_model_train_mode() -> None

      Setting the model into training mode


      Parameter
      ---------



   .. py:method:: set_model_eval_mode() -> None

      Setting the model into eval mode


      Parameter
      ---------



   .. py:property:: name


   .. py:method:: sample_entity(n: int) -> List[str]


   .. py:method:: sample_relation(n: int) -> List[str]


   .. py:method:: is_seen(entity: str = None, relation: str = None) -> bool


   .. py:method:: save() -> None


   .. py:method:: get_entity_index(x: str)


   .. py:method:: get_relation_index(x: str)


   .. py:method:: index_triple(head_entity: List[str], relation: List[str], tail_entity: List[str]) -> Tuple[torch.LongTensor, torch.LongTensor, torch.LongTensor]

      Index Triple

      Parameter
      ---------
      head_entity: List[str]

      String representation of selected entities.

      relation: List[str]

      String representation of selected relations.

      tail_entity: List[str]

      String representation of selected entities.

      Returns: Tuple
      ---------

      pytorch tensor of triple score



   .. py:method:: add_new_entity_embeddings(entity_name: str = None, embeddings: torch.FloatTensor = None)


   .. py:method:: get_entity_embeddings(items: List[str])

      Return embedding of an entity given its string representation


      Parameter
      ---------
      items:
          entities



   .. py:method:: get_relation_embeddings(items: List[str])

      Return embedding of a relation given its string representation


      Parameter
      ---------
      items:
          relations



   .. py:method:: construct_input_and_output(head_entity: List[str], relation: List[str], tail_entity: List[str], labels)

      Construct a data point
      :param head_entity:
      :param relation:
      :param tail_entity:
      :param labels:
      :return:



   .. py:method:: parameters()


.. py:class:: InteractiveQueryDecomposition

   .. py:method:: t_norm(tens_1: torch.Tensor, tens_2: torch.Tensor, tnorm: str = 'min') -> torch.Tensor


   .. py:method:: tensor_t_norm(subquery_scores: torch.FloatTensor, tnorm: str = 'min') -> torch.FloatTensor

      Compute T-norm over [0,1] ^{n   imes d} where n denotes the number of hops and d denotes number of entities



   .. py:method:: t_conorm(tens_1: torch.Tensor, tens_2: torch.Tensor, tconorm: str = 'min') -> torch.Tensor


   .. py:method:: negnorm(tens_1: torch.Tensor, lambda_: float, neg_norm: str = 'standard') -> torch.Tensor


.. py:class:: AbstractCallback

   Bases: :py:obj:`abc.ABC`, :py:obj:`lightning.pytorch.callbacks.Callback`


   Abstract class for Callback class for knowledge graph embedding models


   Parameter
   ---------



   .. py:method:: on_init_start(*args, **kwargs)

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_init_end(*args, **kwargs)

      Call at the beginning of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_fit_start(trainer, model)

      Call at the beginning of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_train_epoch_end(trainer, model)

      Call at the end of each epoch during training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_train_batch_end(*args, **kwargs)

      Call at the end of each mini-batch during the training.


      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_fit_end(*args, **kwargs)

      Call at the end of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



.. py:class:: AbstractPPECallback(num_epochs, path, epoch_to_start, last_percent_to_consider)

   Bases: :py:obj:`AbstractCallback`


   Abstract class for Callback class for knowledge graph embedding models


   Parameter
   ---------



   .. py:attribute:: num_epochs


   .. py:attribute:: path


   .. py:attribute:: sample_counter
      :value: 0



   .. py:attribute:: epoch_count
      :value: 0



   .. py:attribute:: alphas
      :value: None



   .. py:method:: on_fit_start(trainer, model)

      Call at the beginning of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_fit_end(trainer, model)

      Call at the end of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: store_ensemble(param_ensemble) -> None


.. py:class:: BaseInteractiveTrainKGE

   Abstract/base class for training knowledge graph embedding models interactively.
   This class provides methods for re-training KGE models and also Literal Embedding model.


   .. py:method:: train_triples(h: List[str], r: List[str], t: List[str], labels: List[float], iteration=2, optimizer=None)


   .. py:method:: train_k_vs_all(h, r, iteration=1, lr=0.001)

      Train k vs all
      :param head_entity:
      :param relation:
      :param iteration:
      :param lr:
      :return:



   .. py:method:: train(kg, lr=0.1, epoch=10, batch_size=32, neg_sample_ratio=10, num_workers=1) -> None

      Retrained a pretrain model on an input KG via negative sampling.



   .. py:method:: train_literals(train_file_path: str = None, num_epochs: int = 100, lit_lr: float = 0.001, lit_normalization_type: str = 'z-norm', batch_size: int = 1024, sampling_ratio: float = None, random_seed=1, loader_backend: str = 'pandas', freeze_entity_embeddings: bool = True, gate_residual: bool = True, device: str = None)

      Trains the Literal Embeddings model using literal data.

      :param train_file_path: Path to the training data file.
      :type train_file_path: str
      :param num_epochs: Number of training epochs.
      :type num_epochs: int
      :param lit_lr: Learning rate for the literal model.
      :type lit_lr: float
      :param norm_type: Normalization type to use ('z-norm', 'min-max', or None).
      :type norm_type: str
      :param batch_size: Batch size for training.
      :type batch_size: int
      :param sampling_ratio: Ratio of training triples to use.
      :type sampling_ratio: float
      :param loader_backend: Backend for loading the dataset ('pandas' or 'rdflib').
      :type loader_backend: str
      :param freeze_entity_embeddings: If True, freeze the entity embeddings during training.
      :type freeze_entity_embeddings: bool
      :param gate_residual: If True, use gate residual connections in the model.
      :type gate_residual: bool
      :param device: Device to use for training ('cuda' or 'cpu'). If None, will use available GPU or CPU.
      :type device: str



