dicee
=====

.. py:module:: dicee

.. autoapi-nested-parse::

   DICE Embeddings - Knowledge Graph Embedding Library.

   A library for training and using knowledge graph embedding models
   with support for various scoring techniques and training strategies.

   Submodules:
       evaluation: Model evaluation functions and Evaluator class
       models: KGE model implementations
       trainer: Training orchestration
       scripts: Utility scripts



Submodules
----------

.. toctree::
   :maxdepth: 1

   /autoapi/dicee/__main__/index
   /autoapi/dicee/abstracts/index
   /autoapi/dicee/analyse_experiments/index
   /autoapi/dicee/callbacks/index
   /autoapi/dicee/config/index
   /autoapi/dicee/dataset_classes/index
   /autoapi/dicee/eval_static_funcs/index
   /autoapi/dicee/evaluation/index
   /autoapi/dicee/evaluator/index
   /autoapi/dicee/executer/index
   /autoapi/dicee/knowledge_graph/index
   /autoapi/dicee/knowledge_graph_embeddings/index
   /autoapi/dicee/models/index
   /autoapi/dicee/query_generator/index
   /autoapi/dicee/read_preprocess_save_load_kg/index
   /autoapi/dicee/sanity_checkers/index
   /autoapi/dicee/scripts/index
   /autoapi/dicee/static_funcs/index
   /autoapi/dicee/static_funcs_training/index
   /autoapi/dicee/static_preprocess_funcs/index
   /autoapi/dicee/trainer/index
   /autoapi/dicee/weight_averaging/index


Attributes
----------

.. autoapisummary::

   dicee.__version__


Classes
-------

.. autoapisummary::

   dicee.Execute
   dicee.KGE
   dicee.QueryGenerator
   dicee.DICE_Trainer
   dicee.Evaluator


Package Contents
----------------

.. py:class:: Execute(args, continuous_training: bool = False)

   Executor class for training, retraining and evaluating KGE models.

   Handles the complete workflow:
   1. Loading & Preprocessing & Serializing input data
   2. Training & Validation & Testing
   3. Storing all necessary information

   .. attribute:: args

      Processed input arguments.

   .. attribute:: distributed

      Whether distributed training is enabled.

   .. attribute:: rank

      Process rank in distributed training.

   .. attribute:: world_size

      Total number of processes.

   .. attribute:: local_rank

      Local GPU rank.

   .. attribute:: trainer

      Training handler instance.

   .. attribute:: trained_model

      The trained model after training completes.

   .. attribute:: knowledge_graph

      The loaded knowledge graph.

   .. attribute:: report

      Dictionary storing training metrics and results.

   .. attribute:: evaluator

      Model evaluation handler.


   .. py:attribute:: distributed


   .. py:attribute:: args


   .. py:attribute:: is_continual_training
      :value: False



   .. py:attribute:: trainer
      :type:  Optional[dicee.trainer.DICE_Trainer]
      :value: None



   .. py:attribute:: trained_model
      :value: None



   .. py:attribute:: knowledge_graph
      :type:  Optional[dicee.knowledge_graph.KG]
      :value: None



   .. py:attribute:: report
      :type:  Dict


   .. py:attribute:: evaluator
      :type:  Optional[dicee.evaluator.Evaluator]
      :value: None



   .. py:attribute:: start_time
      :type:  Optional[float]
      :value: None



   .. py:method:: is_rank_zero() -> bool


   .. py:method:: cleanup()


   .. py:method:: setup_executor() -> None

      Set up storage directories for the experiment.

      Creates or reuses experiment directories based on configuration.
      Saves the configuration to a JSON file.



   .. py:method:: create_and_store_kg() -> None

      Create knowledge graph and store as memory-mapped file.

      Only executed on rank 0 in distributed training.
      Skips if memmap already exists.



   .. py:method:: load_from_memmap() -> None

      Load knowledge graph from memory-mapped file.



   .. py:method:: save_trained_model() -> None

      Save a knowledge graph embedding model

      (1) Send model to eval mode and cpu.
      (2) Store the memory footprint of the model.
      (3) Save the model into disk.
      (4) Update the stats of KG again ?

      Parameter
      ----------

      :rtype: None



   .. py:method:: end(form_of_labelling: str) -> dict

      End training

      (1) Store trained model.
      (2) Report runtimes.
      (3) Eval model if required.

      Parameter
      ---------

      :rtype: A dict containing information about the training and/or evaluation



   .. py:method:: write_report() -> None

      Report training related information in a report.json file



   .. py:method:: start() -> dict

      Start training

      # (1) Loading the Data
      # (2) Create an evaluator object.
      # (3) Create a trainer object.
      # (4) Start the training

      Parameter
      ---------

      :rtype: A dict containing information about the training and/or evaluation



.. py:class:: KGE(path=None, url=None, construct_ensemble=False, model_name=None)

   Bases: :py:obj:`dicee.abstracts.BaseInteractiveKGE`, :py:obj:`dicee.abstracts.InteractiveQueryDecomposition`, :py:obj:`dicee.abstracts.BaseInteractiveTrainKGE`


   Knowledge Graph Embedding Class for interactive usage of pre-trained models


   .. py:method:: __str__()


   .. py:method:: to(device: str) -> None


   .. py:method:: get_transductive_entity_embeddings(indices: Union[torch.LongTensor, List[str]], as_pytorch=False, as_numpy=False, as_list=True) -> Union[torch.FloatTensor, numpy.ndarray, List[float]]


   .. py:method:: create_vector_database(collection_name: str, distance: str, location: str = 'localhost', port: int = 6333)


   .. py:method:: generate(h='', r='')


   .. py:method:: eval_lp_performance(dataset=List[Tuple[str, str, str]], filtered=True)


   .. py:method:: predict_missing_head_entity(relation: Union[List[str], str], tail_entity: Union[List[str], str], within=None, batch_size=2, topk=1, return_indices=False) -> Tuple

      Given a relation and a tail entity, return top k ranked head entity.

      argmax_{e \in E } f(e,r,t), where r \in R, t \in E.

      Parameter
      ---------
      relation:  Union[List[str], str]

      String representation of selected relations.

      tail_entity: Union[List[str], str]

      String representation of selected entities.


      k: int

      Highest ranked k entities.

      Returns: Tuple
      ---------

      Highest K scores and entities



   .. py:method:: predict_missing_relations(head_entity: Union[List[str], str], tail_entity: Union[List[str], str], within=None, batch_size=2, topk=1, return_indices=False) -> Tuple

      Given a head entity and a tail entity, return top k ranked relations.

      argmax_{r \in R } f(h,r,t), where h, t \in E.


      Parameter
      ---------
      head_entity: List[str]

      String representation of selected entities.

      tail_entity: List[str]

      String representation of selected entities.


      k: int

      Highest ranked k entities.

      Returns: Tuple
      ---------

      Highest K scores and entities



   .. py:method:: predict_missing_tail_entity(head_entity: Union[List[str], str], relation: Union[List[str], str], within: List[str] = None, batch_size=2, topk=1, return_indices=False) -> torch.FloatTensor

      Given a head entity and a relation, return top k ranked entities

      argmax_{e \in E } f(h,r,e), where h \in E and r \in R.


      Parameter
      ---------
      head_entity: List[str]

      String representation of selected entities.

      tail_entity: List[str]

      String representation of selected entities.

      Returns: Tuple
      ---------

      scores



   .. py:method:: predict(*, h: Union[List[str], str] = None, r: Union[List[str], str] = None, t: Union[List[str], str] = None, within=None, logits=True) -> torch.FloatTensor

      :param logits:
      :param h:
      :param r:
      :param t:
      :param within:



   .. py:method:: predict_topk(*, h: Union[str, List[str]] = None, r: Union[str, List[str]] = None, t: Union[str, List[str]] = None, topk: int = 10, within: List[str] = None, batch_size: int = 1024)

      Predict missing item in a given triple.

      :returns:

                - If you query a single (h, r, ?) or (?, r, t) or (h, ?, t), returns List[(item, score)]
                - If you query a batch of B, returns List of B such lists.



   .. py:method:: triple_score(h: Union[List[str], str] = None, r: Union[List[str], str] = None, t: Union[List[str], str] = None, logits=False) -> torch.FloatTensor

      Predict triple score

      Parameter
      ---------
      head_entity: List[str]

      String representation of selected entities.

      relation: List[str]

      String representation of selected relations.

      tail_entity: List[str]

      String representation of selected entities.

      logits: bool

      If logits is True, unnormalized score returned

      Returns: Tuple
      ---------

      pytorch tensor of triple score



   .. py:method:: return_multi_hop_query_results(aggregated_query_for_all_entities, k: int, only_scores)


   .. py:method:: single_hop_query_answering(query: tuple, only_scores: bool = True, k: int = None)


   .. py:method:: answer_multi_hop_query(query_type: str = None, query: Tuple[Union[str, Tuple[str, str]], Ellipsis] = None, queries: List[Tuple[Union[str, Tuple[str, str]], Ellipsis]] = None, tnorm: str = 'prod', neg_norm: str = 'standard', lambda_: float = 0.0, k: int = 10, only_scores=False) -> List[Tuple[str, torch.Tensor]]

      # @TODO: Refactoring is needed
      # @TODO: Score computation for each query type should be done in a static function

      Find an answer set for EPFO queries including negation and disjunction

      Parameter
      ----------
      query_type: str
      The type of the query, e.g., "2p".

      query: Union[str, Tuple[str, Tuple[str, str]]]
      The query itself, either a string or a nested tuple.

      queries: List of Tuple[Union[str, Tuple[str, str]], ...]

      tnorm: str
      The t-norm operator.

      neg_norm: str
      The negation norm.

      lambda_: float
      lambda parameter for sugeno and yager negation norms

      k: int
      The top-k substitutions for intermediate variables.

      :returns: * *List[Tuple[str, torch.Tensor]]*
                * *Entities and corresponding scores sorted in the descening order of scores*



   .. py:method:: find_missing_triples(confidence: float, entities: List[str] = None, relations: List[str] = None, topk: int = 10, at_most: int = sys.maxsize) -> Set

               Find missing triples

               Iterative over a set of entities E and a set of relation R :
      orall e \in E and
      orall r \in R f(e,r,x)
               Return (e,r,x)
      ot\in G and  f(e,r,x) > confidence

              Parameter
              ---------
              confidence: float

              A threshold for an output of a sigmoid function given a triple.

              topk: int

              Highest ranked k item to select triples with f(e,r,x) > confidence .

              at_most: int

              Stop after finding at_most missing triples

              Returns: Set
              ---------

              {(e,r,x) | f(e,r,x) > confidence \land (e,r,x)
      ot\in G




   .. py:method:: predict_literals(entity: Union[List[str], str] = None, attribute: Union[List[str], str] = None, denormalize_preds: bool = True) -> numpy.ndarray

      Predicts literal values for given entities and attributes.

      :param entity: Entity or list of entities to predict literals for.
      :type entity: Union[List[str], str]
      :param attribute: Attribute or list of attributes to predict literals for.
      :type attribute: Union[List[str], str]
      :param denormalize_preds: If True, denormalizes the predictions.
      :type denormalize_preds: bool

      :returns: Predictions for the given entities and attributes.
      :rtype: numpy ndarray



.. py:class:: QueryGenerator(train_path, val_path: str, test_path: str, ent2id: Dict = None, rel2id: Dict = None, seed: int = 1, gen_valid: bool = False, gen_test: bool = True)

   .. py:attribute:: train_path


   .. py:attribute:: val_path


   .. py:attribute:: test_path


   .. py:attribute:: gen_valid
      :value: False



   .. py:attribute:: gen_test
      :value: True



   .. py:attribute:: seed
      :value: 1



   .. py:attribute:: max_ans_num
      :value: 1000000.0



   .. py:attribute:: mode


   .. py:attribute:: ent2id
      :value: None



   .. py:attribute:: rel2id
      :type:  Dict
      :value: None



   .. py:attribute:: ent_in
      :type:  Dict


   .. py:attribute:: ent_out
      :type:  Dict


   .. py:attribute:: query_name_to_struct


   .. py:method:: list2tuple(list_data)


   .. py:method:: tuple2list(x: Union[List, Tuple]) -> Union[List, Tuple]

      Convert a nested tuple to a nested list.



   .. py:method:: set_global_seed(seed: int)

      Set seed



   .. py:method:: construct_graph(paths: List[str]) -> Tuple[Dict, Dict]

      Construct graph from triples
      Returns dicts with incoming and outgoing edges



   .. py:method:: fill_query(query_structure: List[Union[str, List]], ent_in: Dict, ent_out: Dict, answer: int) -> bool

      Private method for fill_query logic.



   .. py:method:: achieve_answer(query: List[Union[str, List]], ent_in: Dict, ent_out: Dict) -> set

      Private method for achieve_answer logic.
      @TODO: Document the code



   .. py:method:: write_links(ent_out, small_ent_out)


   .. py:method:: ground_queries(query_structure: List[Union[str, List]], ent_in: Dict, ent_out: Dict, small_ent_in: Dict, small_ent_out: Dict, gen_num: int, query_name: str)

      Generating queries and achieving answers



   .. py:method:: unmap(query_type, queries, tp_answers, fp_answers, fn_answers)


   .. py:method:: unmap_query(query_structure, query, id2ent, id2rel)


   .. py:method:: generate_queries(query_struct: List, gen_num: int, query_type: str)

      Passing incoming and outgoing edges to ground queries depending on mode [train valid or text]
      and getting queries and answers in return
      @ TODO: create a class for each single query struct



   .. py:method:: save_queries(query_type: str, gen_num: int, save_path: str)

      



   .. py:method:: load_queries(path)
      :abstractmethod:



   .. py:method:: get_queries(query_type: str, gen_num: int)


   .. py:method:: save_queries_and_answers(path: str, data: List[Tuple[str, Tuple[collections.defaultdict]]]) -> None
      :staticmethod:


      Save Queries into Disk



   .. py:method:: load_queries_and_answers(path: str) -> List[Tuple[str, Tuple[collections.defaultdict]]]
      :staticmethod:


      Load Queries from Disk to Memory



.. py:class:: DICE_Trainer(args, is_continual_training: bool, storage_path, evaluator=None)

   DICE_Trainer implement
    1- Pytorch Lightning trainer (https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html)
    2- Multi-GPU Trainer(https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
    3- CPU Trainer

    Parameter
    ---------
    args

    is_continual_training:bool

    storage_path:str

    evaluator:

    Returns
    -------
    report:dict



   .. py:attribute:: report


   .. py:attribute:: args


   .. py:attribute:: trainer
      :value: None



   .. py:attribute:: is_continual_training


   .. py:attribute:: storage_path


   .. py:attribute:: evaluator
      :value: None



   .. py:attribute:: form_of_labelling
      :value: None



   .. py:method:: continual_start(knowledge_graph)

      (1) Initialize training.
      (2) Load model
      (3) Load trainer
      (3) Fit model

      Parameter
      ---------

      :returns: * *model*
                * **form_of_labelling** (*str*)



   .. py:method:: initialize_trainer(callbacks: List) -> lightning.Trainer | dicee.trainer.torch_trainer.TorchTrainer | dicee.trainer.torch_trainer_ddp.TorchDDPTrainer

      Initialize Trainer from input arguments



   .. py:method:: initialize_or_load_model()


   .. py:method:: init_dataloader(dataset: torch.utils.data.Dataset) -> torch.utils.data.DataLoader


   .. py:method:: init_dataset() -> torch.utils.data.Dataset


   .. py:method:: start(knowledge_graph: Union[dicee.knowledge_graph.KG, numpy.memmap]) -> Tuple[dicee.models.base_model.BaseKGE, str]

      Start the training

      (1) Initialize Trainer
      (2) Initialize or load a pretrained KGE model

      in DDP setup, we need to load the memory map of already read/index KG.



   .. py:method:: k_fold_cross_validation(dataset) -> Tuple[dicee.models.base_model.BaseKGE, str]

      Perform K-fold Cross-Validation

      1. Obtain K train and test splits.
      2. For each split,
          2.1 initialize trainer and model
          2.2. Train model with configuration provided in args.
          2.3. Compute the mean reciprocal rank (MRR) score of the model on the test respective split.
      3. Report the mean and average MRR .

      :param self:
      :param dataset:
      :return: model



.. py:class:: Evaluator(args, is_continual_training: bool = False)

   Evaluator class for KGE models in various downstream tasks.

   Orchestrates link prediction evaluation with different scoring techniques
   including standard evaluation and byte-pair encoding based evaluation.

   .. attribute:: er_vocab

      Entity-relation to tail vocabulary for filtered ranking.

   .. attribute:: re_vocab

      Relation-entity (tail) to head vocabulary.

   .. attribute:: ee_vocab

      Entity-entity to relation vocabulary.

   .. attribute:: num_entities

      Total number of entities in the knowledge graph.

   .. attribute:: num_relations

      Total number of relations in the knowledge graph.

   .. attribute:: args

      Configuration arguments.

   .. attribute:: report

      Dictionary storing evaluation results.

   .. attribute:: during_training

      Whether evaluation is happening during training.

   .. rubric:: Example

   >>> from dicee.evaluation import Evaluator
   >>> evaluator = Evaluator(args)
   >>> results = evaluator.eval(dataset, model, 'EntityPrediction')
   >>> print(f"Test MRR: {results['Test']['MRR']:.4f}")


   .. py:attribute:: re_vocab
      :type:  Optional[Dict]
      :value: None



   .. py:attribute:: er_vocab
      :type:  Optional[Dict]
      :value: None



   .. py:attribute:: ee_vocab
      :type:  Optional[Dict]
      :value: None



   .. py:attribute:: func_triple_to_bpe_representation
      :value: None



   .. py:attribute:: is_continual_training
      :value: False



   .. py:attribute:: num_entities
      :type:  Optional[int]
      :value: None



   .. py:attribute:: num_relations
      :type:  Optional[int]
      :value: None



   .. py:attribute:: domain_constraints_per_rel
      :value: None



   .. py:attribute:: range_constraints_per_rel
      :value: None



   .. py:attribute:: args


   .. py:attribute:: report
      :type:  Dict


   .. py:attribute:: during_training
      :value: False



   .. py:method:: vocab_preparation(dataset) -> None

      Prepare vocabularies from the dataset for evaluation.

      Resolves any future objects and saves vocabularies to disk.

      :param dataset: Knowledge graph dataset with vocabulary attributes.



   .. py:method:: eval(dataset, trained_model, form_of_labelling: str, during_training: bool = False) -> Optional[Dict]

      Evaluate the trained model on the dataset.

      :param dataset: Knowledge graph dataset (KG instance).
      :param trained_model: The trained KGE model.
      :param form_of_labelling: Type of labelling ('EntityPrediction' or 'RelationPrediction').
      :param during_training: Whether evaluation is during training.

      :returns: Dictionary of evaluation metrics, or None if evaluation is skipped.



   .. py:method:: eval_rank_of_head_and_tail_entity(*, train_set, valid_set=None, test_set=None, trained_model) -> None

      Evaluate with negative sampling scoring.



   .. py:method:: eval_rank_of_head_and_tail_byte_pair_encoded_entity(*, train_set=None, valid_set=None, test_set=None, ordered_bpe_entities, trained_model) -> None

      Evaluate with BPE-encoded entities and negative sampling.



   .. py:method:: eval_with_byte(*, raw_train_set, raw_valid_set=None, raw_test_set=None, trained_model, form_of_labelling) -> None

      Evaluate BytE model with generation.



   .. py:method:: eval_with_bpe_vs_all(*, raw_train_set, raw_valid_set=None, raw_test_set=None, trained_model, form_of_labelling) -> None

      Evaluate with BPE and KvsAll scoring.



   .. py:method:: eval_with_vs_all(*, train_set, valid_set=None, test_set=None, trained_model, form_of_labelling) -> None

      Evaluate with KvsAll or 1vsAll scoring.



   .. py:method:: evaluate_lp_k_vs_all(model, triple_idx, info: Optional[str] = None, form_of_labelling: Optional[str] = None) -> Dict[str, float]

      Filtered link prediction evaluation with KvsAll scoring.

      :param model: The trained model to evaluate.
      :param triple_idx: Integer-indexed test triples.
      :param info: Description to print.
      :param form_of_labelling: 'EntityPrediction' or 'RelationPrediction'.

      :returns: Dictionary with H@1, H@3, H@10, and MRR metrics.



   .. py:method:: evaluate_lp_with_byte(model, triples: List[List[str]], info: Optional[str] = None) -> Dict[str, float]

      Evaluate BytE model with text generation.

      :param model: BytE model.
      :param triples: String triples.
      :param info: Description to print.

      :returns: Dictionary with placeholder metrics (-1 values).



   .. py:method:: evaluate_lp_bpe_k_vs_all(model, triples: List[List[str]], info: Optional[str] = None, form_of_labelling: Optional[str] = None) -> Dict[str, float]

      Evaluate BPE model with KvsAll scoring.

      :param model: BPE-enabled model.
      :param triples: String triples.
      :param info: Description to print.
      :param form_of_labelling: Type of labelling.

      :returns: Dictionary with H@1, H@3, H@10, and MRR metrics.



   .. py:method:: evaluate_lp(model, triple_idx, info: str) -> Dict[str, float]

      Evaluate link prediction with negative sampling.

      :param model: The model to evaluate.
      :param triple_idx: Integer-indexed triples.
      :param info: Description to print.

      :returns: Dictionary with H@1, H@3, H@10, and MRR metrics.



   .. py:method:: dummy_eval(trained_model, form_of_labelling: str) -> None

      Run evaluation from saved data (for continual training).

      :param trained_model: The trained model.
      :param form_of_labelling: Type of labelling.



   .. py:method:: eval_with_data(dataset, trained_model, triple_idx: numpy.ndarray, form_of_labelling: str) -> Dict[str, float]

      Evaluate a trained model on a given dataset.

      :param dataset: Knowledge graph dataset.
      :param trained_model: The trained model.
      :param triple_idx: Integer-indexed triples to evaluate.
      :param form_of_labelling: Type of labelling.

      :returns: Dictionary with evaluation metrics.

      :raises ValueError: If scoring technique is invalid.



.. py:data:: __version__
   :value: '0.3.2'


