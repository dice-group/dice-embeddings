dicee.callbacks
===============

.. py:module:: dicee.callbacks


Classes
-------

.. autoapisummary::

   dicee.callbacks.AccumulateEpochLossCallback
   dicee.callbacks.PrintCallback
   dicee.callbacks.KGESaveCallback
   dicee.callbacks.PseudoLabellingCallback
   dicee.callbacks.ASWA
   dicee.callbacks.Eval
   dicee.callbacks.KronE
   dicee.callbacks.Perturb
   dicee.callbacks.PeriodicEvalCallback
   dicee.callbacks.LRScheduler
   dicee.callbacks.SWA


Functions
---------

.. autoapisummary::

   dicee.callbacks.estimate_q
   dicee.callbacks.compute_convergence


Module Contents
---------------

.. py:class:: AccumulateEpochLossCallback(path: str)

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   Abstract class for Callback class for knowledge graph embedding models


   Parameter
   ---------



   .. py:attribute:: path


   .. py:method:: on_fit_end(trainer, model) -> None

      Store epoch loss


      Parameter
      ---------
      trainer:

      model:

      :rtype: None



.. py:class:: PrintCallback

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   Abstract class for Callback class for knowledge graph embedding models


   Parameter
   ---------



   .. py:attribute:: start_time


   .. py:method:: on_fit_start(trainer, pl_module)

      Call at the beginning of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_fit_end(trainer, pl_module)

      Call at the end of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_train_batch_end(*args, **kwargs)

      Call at the end of each mini-batch during the training.


      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_train_epoch_end(*args, **kwargs)

      Call at the end of each epoch during training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



.. py:class:: KGESaveCallback(every_x_epoch: int, max_epochs: int, path: str)

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   Abstract class for Callback class for knowledge graph embedding models


   Parameter
   ---------



   .. py:attribute:: every_x_epoch


   .. py:attribute:: max_epochs


   .. py:attribute:: epoch_counter
      :value: 0



   .. py:attribute:: path


   .. py:method:: on_train_batch_end(*args, **kwargs)

      Call at the end of each mini-batch during the training.


      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_fit_start(trainer, pl_module)

      Call at the beginning of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_train_epoch_end(*args, **kwargs)

      Call at the end of each epoch during training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_fit_end(*args, **kwargs)

      Call at the end of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_epoch_end(model, trainer, **kwargs)


.. py:class:: PseudoLabellingCallback(data_module, kg, batch_size)

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   Abstract class for Callback class for knowledge graph embedding models


   Parameter
   ---------



   .. py:attribute:: data_module


   .. py:attribute:: kg


   .. py:attribute:: num_of_epochs
      :value: 0



   .. py:attribute:: unlabelled_size


   .. py:attribute:: batch_size


   .. py:method:: create_random_data()


   .. py:method:: on_epoch_end(trainer, model)


.. py:function:: estimate_q(eps)

   estimate rate of convergence q from sequence esp


.. py:function:: compute_convergence(seq, i)

.. py:class:: ASWA(num_epochs, path)

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   Adaptive stochastic weight averaging
   ASWE keeps track of the validation performance and update s the ensemble model accordingly.


   .. py:attribute:: path


   .. py:attribute:: num_epochs


   .. py:attribute:: initial_eval_setting
      :value: None



   .. py:attribute:: epoch_count
      :value: 0



   .. py:attribute:: alphas
      :value: []



   .. py:attribute:: val_aswa
      :value: -1



   .. py:method:: on_fit_end(trainer, model)

      Call at the end of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: compute_mrr(trainer, model) -> float
      :staticmethod:



   .. py:method:: get_aswa_state_dict(model)


   .. py:method:: decide(running_model_state_dict, ensemble_state_dict, val_running_model, mrr_updated_ensemble_model)

      Perform Hard Update, software or rejection

      :param running_model_state_dict:
      :param ensemble_state_dict:
      :param val_running_model:
      :param mrr_updated_ensemble_model:



   .. py:method:: on_train_epoch_end(trainer, model)

      Call at the end of each epoch during training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



.. py:class:: Eval(path, epoch_ratio: int = None)

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   Abstract class for Callback class for knowledge graph embedding models


   Parameter
   ---------



   .. py:attribute:: path


   .. py:attribute:: reports
      :value: []



   .. py:attribute:: epoch_ratio
      :value: None



   .. py:attribute:: epoch_counter
      :value: 0



   .. py:method:: on_fit_start(trainer, model)

      Call at the beginning of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_fit_end(trainer, model)

      Call at the end of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_train_epoch_end(trainer, model)

      Call at the end of each epoch during training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_train_batch_end(*args, **kwargs)

      Call at the end of each mini-batch during the training.


      Parameter
      ---------
      trainer:

      model:

      :rtype: None



.. py:class:: KronE

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   Abstract class for Callback class for knowledge graph embedding models


   Parameter
   ---------



   .. py:attribute:: f
      :value: None



   .. py:method:: batch_kronecker_product(a, b)
      :staticmethod:


      Kronecker product of matrices a and b with leading batch dimensions.
      Batch dimensions are broadcast. The number of them mush
      :type a: torch.Tensor
      :type b: torch.Tensor
      :rtype: torch.Tensor



   .. py:method:: get_kronecker_triple_representation(indexed_triple: torch.LongTensor)

      Get kronecker embeddings



   .. py:method:: on_fit_start(trainer, model)

      Call at the beginning of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



.. py:class:: Perturb(level: str = 'input', ratio: float = 0.0, method: str = None, scaler: float = None, frequency=None)

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   A callback for a three-Level Perturbation

   Input Perturbation: During training an input x is perturbed by randomly replacing its element.
   In the context of knowledge graph embedding models, x can denote a triple, a tuple of an entity and a relation,
   or a tuple of two entities.
   A perturbation means that a component of x is randomly replaced by an entity or a relation.

   Parameter Perturbation:

   Output Perturbation:


   .. py:attribute:: level
      :value: 'input'



   .. py:attribute:: ratio
      :value: 0.0



   .. py:attribute:: method
      :value: None



   .. py:attribute:: scaler
      :value: None



   .. py:attribute:: frequency
      :value: None



   .. py:method:: on_train_batch_start(trainer, model, batch, batch_idx)

      Called when the train batch begins.



.. py:class:: PeriodicEvalCallback(experiment_path: str, max_epochs: int, eval_every_n_epoch: int = 0, eval_at_epochs: list = None, save_model_every_n_epoch: bool = True, n_epochs_eval_model: str = 'val_test')

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   Callback to periodically evaluate the model and optionally save checkpoints during training.

   Evaluates at regular intervals (every N epochs) or at explicitly specified epochs.
   Stores evaluation reports and model checkpoints.


   .. py:attribute:: experiment_dir


   .. py:attribute:: max_epochs


   .. py:attribute:: epoch_counter
      :value: 0



   .. py:attribute:: save_model_every_n_epoch
      :value: True



   .. py:attribute:: reports


   .. py:attribute:: n_epochs_eval_model
      :value: 'val_test'



   .. py:attribute:: default_eval_model
      :value: None



   .. py:attribute:: eval_epochs


   .. py:method:: on_fit_end(trainer, model)

      Called at the end of training. Saves final evaluation report.



   .. py:method:: on_train_epoch_end(trainer, model)

      Called at the end of each training epoch. Performs evaluation and checkpointing if scheduled.

      :param trainer: The training controller.
      :type trainer: object
      :param model: The model being trained.
      :type model: torch.nn.Module



.. py:class:: LRScheduler(adaptive_lr_config: dict, total_epochs: int, experiment_dir: str, eta_max: float = 0.1, snapshot_dir: str = 'snapshots')

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   Callback for managing learning rate scheduling and model snapshots.

   Supports cosine annealing ("cca"), MMCCLR ("mmcclr"), and their deferred (warmup) variants:
   - "deferred_cca"
   - "deferred_mmcclr"

   At the end of each learning rate cycle, the model can optionally be saved as a snapshot.


   .. py:attribute:: total_epochs


   .. py:attribute:: experiment_dir


   .. py:attribute:: snapshot_dir


   .. py:attribute:: batches_per_epoch
      :value: None



   .. py:attribute:: total_steps
      :value: None



   .. py:attribute:: cycle_length
      :value: None



   .. py:attribute:: warmup_steps
      :value: None



   .. py:attribute:: lr_lambda
      :value: None



   .. py:attribute:: scheduler
      :value: None



   .. py:attribute:: step_count
      :value: 0



   .. py:attribute:: snapshot_loss


   .. py:method:: on_train_start(trainer, model)

      Initialize training parameters and LR scheduler at start of training.



   .. py:method:: on_train_batch_end(trainer, model, outputs, batch, batch_idx)

      Step the LR scheduler and save model snapshot if needed after each batch.



   .. py:method:: on_fit_end(trainer, model)

      Call at the end of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



.. py:class:: SWA(swa_start_epoch, swa_c_epochs: int = 1, lr_init: float = 0.1, swa_lr: float = 0.05, max_epochs: int = None)

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   Stochastic Weight Averaging callbacks.


   .. py:attribute:: swa_start_epoch


   .. py:attribute:: swa_c_epochs
      :value: 1



   .. py:attribute:: swa_lr
      :value: 0.05



   .. py:attribute:: lr_init
      :value: 0.1



   .. py:attribute:: max_epochs
      :value: None



   .. py:attribute:: swa_model
      :value: None



   .. py:attribute:: swa_n
      :value: 0



   .. py:attribute:: current_epoch
      :value: -1



   .. py:method:: moving_average(swa_model, model, alpha)

      Update SWA model with moving average of current model.



   .. py:method:: on_fit_start(trainer, model)

      Initialize SWA model with same architecture as main model.



   .. py:method:: on_train_epoch_start(trainer, model)

      Update learning rate according to SWA schedule.



   .. py:method:: on_train_epoch_end(trainer, model)

      Apply SWA averaging if conditions are met.



   .. py:method:: on_fit_end(trainer, model)

      Replace main model with SWA model at the end of training.



