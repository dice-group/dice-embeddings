

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>dicee.models.adopt &mdash; DICE Embeddings 0.1.3.2 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=677a9ea0" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css?v=4ae1632d" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/theme.css?v=ea877efc" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/theme_tweak.css?v=f0ad19f3" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=677a9ea0" />

  
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
      <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../_static/documentation_options.js?v=c6726a90"></script>
      <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="dicee.models.base_model" href="../base_model/index.html" />
    <link rel="prev" title="dicee.models" href="../index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            DICE Embeddings
              <img src="../../../../_static/dicee_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html">Dicee Manual</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#download-knowledge-graphs">Download Knowledge Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#knowledge-graph-embedding-models">Knowledge Graph Embedding Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#how-to-train">How to Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#creating-an-embedding-vector-database">Creating an Embedding Vector Database</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#answering-complex-queries">Answering Complex Queries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#predicting-missing-links">Predicting Missing Links</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#downloading-pretrained-models">Downloading Pretrained Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#how-to-deploy">How to Deploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#docker">Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#coverage-report">Coverage Report</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#how-to-cite">How to cite</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">dicee</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../index.html#submodules">Submodules</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../__main__/index.html">dicee.__main__</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../abstracts/index.html">dicee.abstracts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../analyse_experiments/index.html">dicee.analyse_experiments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../callbacks/index.html">dicee.callbacks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../config/index.html">dicee.config</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dataset_classes/index.html">dicee.dataset_classes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../eval_static_funcs/index.html">dicee.eval_static_funcs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../evaluation/index.html">dicee.evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../evaluator/index.html">dicee.evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../executer/index.html">dicee.executer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../knowledge_graph/index.html">dicee.knowledge_graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../knowledge_graph_embeddings/index.html">dicee.knowledge_graph_embeddings</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../index.html">dicee.models</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../index.html#submodules">Submodules</a><ul class="current">
<li class="toctree-l5 current"><a class="current reference internal" href="#">dicee.models.adopt</a><ul>
<li class="toctree-l6"><a class="reference internal" href="#adopt-overview">ADOPT Overview:</a></li>
<li class="toctree-l6"><a class="reference internal" href="#algorithm-comparison">Algorithm Comparison:</a></li>
<li class="toctree-l6"><a class="reference internal" href="#classes">Classes:</a></li>
<li class="toctree-l6"><a class="reference internal" href="#functions">Functions:</a></li>
<li class="toctree-l6"><a class="reference internal" href="#performance">Performance:</a></li>
<li class="toctree-l6"><a class="reference internal" href="#example">Example:</a></li>
<li class="toctree-l6"><a class="reference internal" href="#references">References:</a></li>
<li class="toctree-l6"><a class="reference internal" href="#notes">Notes:</a></li>
<li class="toctree-l6"><a class="reference internal" href="#id1">Classes</a></li>
<li class="toctree-l6"><a class="reference internal" href="#id2">Functions</a></li>
<li class="toctree-l6"><a class="reference internal" href="#module-contents">Module Contents</a></li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="../base_model/index.html">dicee.models.base_model</a></li>
<li class="toctree-l5"><a class="reference internal" href="../clifford/index.html">dicee.models.clifford</a></li>
<li class="toctree-l5"><a class="reference internal" href="../complex/index.html">dicee.models.complex</a></li>
<li class="toctree-l5"><a class="reference internal" href="../dualE/index.html">dicee.models.dualE</a></li>
<li class="toctree-l5"><a class="reference internal" href="../ensemble/index.html">dicee.models.ensemble</a></li>
<li class="toctree-l5"><a class="reference internal" href="../function_space/index.html">dicee.models.function_space</a></li>
<li class="toctree-l5"><a class="reference internal" href="../literal/index.html">dicee.models.literal</a></li>
<li class="toctree-l5"><a class="reference internal" href="../octonion/index.html">dicee.models.octonion</a></li>
<li class="toctree-l5"><a class="reference internal" href="../pykeen_models/index.html">dicee.models.pykeen_models</a></li>
<li class="toctree-l5"><a class="reference internal" href="../quaternion/index.html">dicee.models.quaternion</a></li>
<li class="toctree-l5"><a class="reference internal" href="../real/index.html">dicee.models.real</a></li>
<li class="toctree-l5"><a class="reference internal" href="../static_funcs/index.html">dicee.models.static_funcs</a></li>
<li class="toctree-l5"><a class="reference internal" href="../transformers/index.html">dicee.models.transformers</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../index.html#classes">Classes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../index.html#functions">Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../index.html#package-contents">Package Contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../query_generator/index.html">dicee.query_generator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../read_preprocess_save_load_kg/index.html">dicee.read_preprocess_save_load_kg</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../sanity_checkers/index.html">dicee.sanity_checkers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../scripts/index.html">dicee.scripts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../static_funcs/index.html">dicee.static_funcs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../static_funcs_training/index.html">dicee.static_funcs_training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../static_preprocess_funcs/index.html">dicee.static_preprocess_funcs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../trainer/index.html">dicee.trainer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../weight_averaging/index.html">dicee.weight_averaging</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#attributes">Attributes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#classes">Classes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#package-contents">Package Contents</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">DICE Embeddings</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">dicee</a></li>
          <li class="breadcrumb-item"><a href="../index.html">dicee.models</a></li>
      <li class="breadcrumb-item active">dicee.models.adopt</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/autoapi/dicee/models/adopt/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-dicee.models.adopt">
<span id="dicee-models-adopt"></span><h1>dicee.models.adopt<a class="headerlink" href="#module-dicee.models.adopt" title="Link to this heading"></a></h1>
<p>ADOPT Optimizer Implementation.</p>
<p>This module implements the ADOPT (Adaptive Optimization with Precise Tracking) algorithm,
an advanced optimization method for training neural networks.</p>
<section id="adopt-overview">
<h2>ADOPT Overview:<a class="headerlink" href="#adopt-overview" title="Link to this heading"></a></h2>
<p>ADOPT is an adaptive learning rate optimization algorithm that combines the benefits of
momentum-based methods with per-parameter learning rate adaptation. Unlike Adam, which
applies momentum to raw gradients, ADOPT normalizes gradients first and then applies
momentum, leading to more stable training dynamics.</p>
<p>Key Features:
- Gradient normalization before momentum application
- Adaptive per-parameter learning rates
- Optional gradient clipping that grows with training steps
- Support for decoupled weight decay (AdamW-style)
- Multiple execution modes: single-tensor, multi-tensor (foreach), and fused (planned)</p>
</section>
<section id="algorithm-comparison">
<h2>Algorithm Comparison:<a class="headerlink" href="#algorithm-comparison" title="Link to this heading"></a></h2>
<p>Adam:    m = β₁*m + (1-β₁)*g,  θ = θ - α*m/√v
ADOPT:   m = β₁*m + (1-β₁)*g/√v,  θ = θ - α*m</p>
<p>The key difference is that ADOPT normalizes gradients before momentum, which provides
better stability and can lead to improved convergence.</p>
</section>
<section id="classes">
<h2>Classes:<a class="headerlink" href="#classes" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>ADOPT: Main optimizer class (extends torch.optim.Optimizer)</p></li>
</ul>
</section>
<section id="functions">
<h2>Functions:<a class="headerlink" href="#functions" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>adopt: Functional API for ADOPT algorithm computation</p></li>
<li><p>_single_tensor_adopt: Single-tensor implementation (TorchScript compatible)</p></li>
<li><p>_multi_tensor_adopt: Multi-tensor implementation using foreach operations</p></li>
</ul>
</section>
<section id="performance">
<h2>Performance:<a class="headerlink" href="#performance" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Single-tensor: Default, compatible with torch.jit.script</p></li>
<li><p>Multi-tensor (foreach): 2-3x faster on GPU through vectorization</p></li>
<li><p>Fused (planned): Would provide maximum performance via specialized kernels</p></li>
</ul>
</section>
<section id="example">
<h2>Example:<a class="headerlink" href="#example" title="Link to this heading"></a></h2>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">dicee.models.adopt</span><span class="w"> </span><span class="kn">import</span> <span class="n">ADOPT</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">ADOPT</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">decouple</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Training loop</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="references">
<h2>References:<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<p>Original implementation: <a class="reference external" href="https://github.com/iShohei220/adopt">https://github.com/iShohei220/adopt</a></p>
</section>
<section id="notes">
<h2>Notes:<a class="headerlink" href="#notes" title="Link to this heading"></a></h2>
<p>This implementation is based on the original ADOPT implementation and adapted to work
with the PyTorch optimizer interface and the dicee framework.</p>
</section>
<section id="id1">
<h2>Classes<a class="headerlink" href="#id1" title="Link to this heading"></a></h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#dicee.models.adopt.ADOPT" title="dicee.models.adopt.ADOPT"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ADOPT</span></code></a></p></td>
<td><p>ADOPT  Optimizer.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id2">
<h2>Functions<a class="headerlink" href="#id2" title="Link to this heading"></a></h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#dicee.models.adopt.adopt" title="dicee.models.adopt.adopt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adopt</span></code></a>(params, grads, exp_avgs, exp_avg_sqs, state_steps)</p></td>
<td><p>Functional API that performs ADOPT algorithm computation.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="dicee.models.adopt.ADOPT">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dicee.models.adopt.</span></span><span class="sig-name descname"><span class="pre">ADOPT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.optim.optimizer.ParamsT</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">torch.Tensor</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">betas</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">(0.9,</span> <span class="pre">0.9999)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_lambda</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">lambda</span> <span class="pre">step:</span> <span class="pre">...</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decouple</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>, <em class="sig-param"><span class="n"><span class="pre">foreach</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maximize</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">capturable</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">differentiable</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fused</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dicee.models.adopt.ADOPT" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.optim.optimizer.Optimizer</span></code></p>
<p>ADOPT  Optimizer.</p>
<p>ADOPT is an adaptive learning rate optimization algorithm that combines momentum-based
updates with adaptive per-parameter learning rates. It uses exponential moving averages
of gradients and squared gradients, with gradient clipping for stability.</p>
<p>The algorithm performs the following key operations:
1. Normalizes gradients by the square root of the second moment estimate
2. Applies optional gradient clipping based on the training step
3. Updates parameters using momentum-smoothed normalized gradients
4. Supports decoupled weight decay (AdamW-style) or L2 regularization</p>
<dl class="simple">
<dt>Mathematical formulation:</dt><dd><p>m_t = β₁ * m_{t-1} + (1 - β₁) * clip(g_t / √(v_t))
v_t = β₂ * v_{t-1} + (1 - β₂) * g_t²
θ_t = θ_{t-1} - α * m_t</p>
</dd>
<dt>where:</dt><dd><ul class="simple">
<li><p>θ_t: parameter at step t</p></li>
<li><p>g_t: gradient at step t</p></li>
<li><p>m_t: first moment estimate (momentum)</p></li>
<li><p>v_t: second moment estimate (variance)</p></li>
<li><p>α: learning rate</p></li>
<li><p>β₁, β₂: exponential decay rates</p></li>
<li><p>clip(): optional gradient clipping function</p></li>
</ul>
</dd>
<dt>Reference:</dt><dd><p>Original implementation: <a class="reference external" href="https://github.com/iShohei220/adopt">https://github.com/iShohei220/adopt</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>ParamsT</em>) – Iterable of parameters to optimize or dicts defining parameter groups.</p></li>
<li><p><strong>lr</strong> (<em>float</em><em> or </em><em>Tensor</em><em>, </em><em>optional</em>) – Learning rate. Can be a float or 1-element Tensor.
Default: 1e-3</p></li>
<li><p><strong>betas</strong> (<em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – Coefficients (β₁, β₂) for computing running
averages of gradient and its square. β₁ controls momentum, β₂ controls variance.
Default: (0.9, 0.9999)</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – Term added to denominator to improve numerical stability.
Default: 1e-6</p></li>
<li><p><strong>clip_lambda</strong> (<em>Callable</em><em>[</em><em>[</em><em>int</em><em>]</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – Function that takes the step number
and returns the gradient clipping threshold. Common choices:
- lambda step: step**0.25 (default, gradually increases clipping threshold)
- lambda step: 1.0 (constant clipping)
- None (no clipping)
Default: lambda step: step**0.25</p></li>
<li><p><strong>weight_decay</strong> (<em>float</em><em>, </em><em>optional</em>) – Weight decay coefficient (L2 penalty).
Default: 0.0</p></li>
<li><p><strong>decouple</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, uses decoupled weight decay (AdamW-style),
applying weight decay directly to parameters. If False, adds weight decay
to gradients (L2 regularization). Default: False</p></li>
<li><p><strong>foreach</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, uses the faster foreach implementation for
multi-tensor operations. Default: None (auto-select)</p></li>
<li><p><strong>maximize</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, maximizes parameters instead of minimizing.
Useful for reinforcement learning. Default: False</p></li>
<li><p><strong>capturable</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, the optimizer is safe to capture in a
CUDA graph. Requires learning rate as Tensor. Default: False</p></li>
<li><p><strong>differentiable</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, the optimization step can be
differentiated. Useful for meta-learning. Default: False</p></li>
<li><p><strong>fused</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, uses fused kernel implementation (currently
not supported). Default: None</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – If learning rate, epsilon, betas, or weight_decay are invalid.</p></li>
<li><p><strong>RuntimeError</strong> – If fused is enabled (not currently supported).</p></li>
<li><p><strong>RuntimeError</strong> – If lr is a Tensor with foreach=True and capturable=False.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Basic usage</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">ADOPT</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With decoupled weight decay</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">ADOPT</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">decouple</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Custom gradient clipping</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">ADOPT</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip_lambda</span><span class="o">=</span><span class="k">lambda</span> <span class="n">step</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="o">**</span><span class="mf">0.5</span><span class="p">))</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>For most use cases, the default hyperparameters work well</p></li>
<li><p>Consider using decouple=True for better generalization (similar to AdamW)</p></li>
<li><p>The clip_lambda function helps stabilize training in early steps</p></li>
</ul>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="dicee.models.adopt.ADOPT.clip_lambda">
<span class="sig-name descname"><span class="pre">clip_lambda</span></span><a class="headerlink" href="#dicee.models.adopt.ADOPT.clip_lambda" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dicee.models.adopt.ADOPT.__setstate__">
<span class="sig-name descname"><span class="pre">__setstate__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dicee.models.adopt.ADOPT.__setstate__" title="Link to this definition"></a></dt>
<dd><p>Restore optimizer state from a checkpoint.</p>
<p>This method handles backward compatibility when loading optimizer state from
older versions. It ensures all required fields are present with default values
and properly converts step counters to tensors if needed.</p>
<p>Key responsibilities:
1. Set default values for newly added hyperparameters
2. Convert old-style scalar step counters to tensor format
3. Place step tensors on appropriate devices based on capturable/fused modes</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>state</strong> (<em>dict</em>) – Optimizer state dictionary (typically from torch.load()).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>This enables loading checkpoints saved with older ADOPT versions</p></li>
<li><p>Step counters are converted to appropriate device/dtype for compatibility</p></li>
<li><p>Capturable and fused modes require step tensors on parameter devices</p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dicee.models.adopt.ADOPT.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">closure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dicee.models.adopt.ADOPT.step" title="Link to this definition"></a></dt>
<dd><p>Perform a single optimization step.</p>
<p>This method executes one iteration of the ADOPT optimization algorithm across
all parameter groups. It orchestrates the following workflow:</p>
<ol class="arabic simple">
<li><p>Optionally evaluates a closure to recompute the loss (useful for algorithms
like LBFGS or when loss needs multiple evaluations)</p></li>
<li><p>For each parameter group:
- Collects parameters with gradients and their associated state
- Extracts hyperparameters (betas, learning rate, etc.)
- Calls the functional adopt() API to perform the actual update</p></li>
<li><p>Returns the loss value if a closure was provided</p></li>
</ol>
<p>The functional API (adopt()) handles three execution modes:
- Single-tensor: Updates one parameter at a time (default, JIT-compatible)
- Multi-tensor (foreach): Batches operations for better performance
- Fused: Uses fused CUDA kernels (not yet implemented)</p>
<p>Gradient scaling support:
This method is compatible with automatic mixed precision (AMP) training.
It can access grad_scale and found_inf attributes for gradient unscaling
and inf/nan detection when used with GradScaler.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>closure</strong> (<em>Callable</em><em>, </em><em>optional</em>) – A callable that reevaluates the model and
returns the loss. The closure should:
- Enable gradients (torch.enable_grad())
- Compute forward pass
- Compute loss
- Compute backward pass
- Return the loss value
Example: lambda: (loss := model(x), loss.backward(), loss)[-1]
Default: None</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>The loss value returned by the closure, or None if no</dt><dd><p>closure was provided.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Optional[Tensor]</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Standard usage</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With closure (e.g., for line search)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
<span class="gp">... </span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Call zero_grad() before computing gradients for the next step</p></li>
<li><p>CUDA graph capture is checked for safety when capturable=True</p></li>
<li><p>The method is thread-safe for different parameter groups</p></li>
</ul>
</div>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="dicee.models.adopt.adopt">
<span class="sig-prename descclassname"><span class="pre">dicee.models.adopt.</span></span><span class="sig-name descname"><span class="pre">adopt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_avgs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp_avg_sqs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_steps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">foreach</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">capturable</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">differentiable</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fused</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">found_inf</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_complex</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_lambda</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decouple</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maximize</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dicee.models.adopt.adopt" title="Link to this definition"></a></dt>
<dd><p>Functional API that performs ADOPT algorithm computation.</p>
<p>This is the main functional interface for the ADOPT optimization algorithm. It
dispatches to one of three implementations based on the execution mode:</p>
<ol class="arabic simple">
<li><p><strong>Single-tensor mode</strong> (default): Updates parameters one at a time
- Compatible with torch.jit.script
- More flexible but slower
- Used when foreach=False or automatically for small models</p></li>
<li><p><strong>Multi-tensor (foreach) mode</strong>: Batches operations across tensors
- 2-3x faster on GPU through vectorization
- Groups tensors by device/dtype automatically
- Used when foreach=True</p></li>
<li><p><strong>Fused mode</strong>: Uses specialized fused kernels (not yet implemented)
- Would provide maximum performance
- Currently raises RuntimeError if enabled</p></li>
</ol>
<section id="algorithm-overview-adopt">
<h3>Algorithm overview (ADOPT):<a class="headerlink" href="#algorithm-overview-adopt" title="Link to this heading"></a></h3>
<p>ADOPT adapts learning rates per-parameter while using momentum on normalized
gradients. The key innovation is normalizing gradients before momentum, which
provides more stable training than standard Adam.</p>
<dl>
<dt>Mathematical formulation:</dt><dd><p># Normalize gradient by its historical variance
normed_g_t = g_t / √(v_t + ε)</p>
<p># Optional gradient clipping for stability
normed_g_t = clip(normed_g_t, threshold(t))</p>
<p># Momentum on normalized gradients (key difference from Adam)
m_t = β₁ * m_{t-1} + (1 - β₁) * normed_g_t</p>
<p># Parameter update
θ_t = θ_{t-1} - α * m_t</p>
<p># Update variance estimate
v_t = β₂ * v_{t-1} + (1 - β₂) * g_t²</p>
</dd>
<dt>where:</dt><dd><ul class="simple">
<li><p>θ: parameters</p></li>
<li><p>g: gradients</p></li>
<li><p>m: first moment (momentum of normalized gradients)</p></li>
<li><p>v: second moment (variance of raw gradients)</p></li>
<li><p>α: learning rate</p></li>
<li><p>β₁, β₂: exponential decay rates</p></li>
<li><p>ε: numerical stability constant</p></li>
<li><p>clip(): gradient clipping function based on step</p></li>
</ul>
</dd>
</dl>
</section>
<section id="automatic-mode-selection">
<h3>Automatic mode selection:<a class="headerlink" href="#automatic-mode-selection" title="Link to this heading"></a></h3>
<p>When foreach and fused are both None (default), the function automatically
selects the best implementation based on:
- Parameter types and devices
- Whether differentiable mode is enabled
- Learning rate type (float vs Tensor)
- Capturable mode requirements</p>
<dl class="field-list simple">
<dt class="field-odd">param params<span class="colon">:</span></dt>
<dd class="field-odd"><p>Parameters to optimize.</p>
</dd>
<dt class="field-even">type params<span class="colon">:</span></dt>
<dd class="field-even"><p>List[Tensor]</p>
</dd>
<dt class="field-odd">param grads<span class="colon">:</span></dt>
<dd class="field-odd"><p>Gradients for each parameter.</p>
</dd>
<dt class="field-even">type grads<span class="colon">:</span></dt>
<dd class="field-even"><p>List[Tensor]</p>
</dd>
<dt class="field-odd">param exp_avgs<span class="colon">:</span></dt>
<dd class="field-odd"><p>First moment estimates (momentum).</p>
</dd>
<dt class="field-even">type exp_avgs<span class="colon">:</span></dt>
<dd class="field-even"><p>List[Tensor]</p>
</dd>
<dt class="field-odd">param exp_avg_sqs<span class="colon">:</span></dt>
<dd class="field-odd"><p>Second moment estimates (variance).</p>
</dd>
<dt class="field-even">type exp_avg_sqs<span class="colon">:</span></dt>
<dd class="field-even"><p>List[Tensor]</p>
</dd>
<dt class="field-odd">param state_steps<span class="colon">:</span></dt>
<dd class="field-odd"><p>Step counters (must be singleton tensors).</p>
</dd>
<dt class="field-even">type state_steps<span class="colon">:</span></dt>
<dd class="field-even"><p>List[Tensor]</p>
</dd>
<dt class="field-odd">param foreach<span class="colon">:</span></dt>
<dd class="field-odd"><p>Whether to use multi-tensor implementation.
None: auto-select based on configuration (default).</p>
</dd>
<dt class="field-even">type foreach<span class="colon">:</span></dt>
<dd class="field-even"><p>Optional[bool]</p>
</dd>
<dt class="field-odd">param capturable<span class="colon">:</span></dt>
<dd class="field-odd"><p>If True, ensure CUDA graph capture safety.</p>
</dd>
<dt class="field-even">type capturable<span class="colon">:</span></dt>
<dd class="field-even"><p>bool</p>
</dd>
<dt class="field-odd">param differentiable<span class="colon">:</span></dt>
<dd class="field-odd"><p>If True, allow gradients through optimization step.</p>
</dd>
<dt class="field-even">type differentiable<span class="colon">:</span></dt>
<dd class="field-even"><p>bool</p>
</dd>
<dt class="field-odd">param fused<span class="colon">:</span></dt>
<dd class="field-odd"><p>If True, use fused kernels (not implemented).</p>
</dd>
<dt class="field-even">type fused<span class="colon">:</span></dt>
<dd class="field-even"><p>Optional[bool]</p>
</dd>
<dt class="field-odd">param grad_scale<span class="colon">:</span></dt>
<dd class="field-odd"><p>Gradient scaler for AMP training.</p>
</dd>
<dt class="field-even">type grad_scale<span class="colon">:</span></dt>
<dd class="field-even"><p>Optional[Tensor]</p>
</dd>
<dt class="field-odd">param found_inf<span class="colon">:</span></dt>
<dd class="field-odd"><p>Flag for inf/nan detection in AMP.</p>
</dd>
<dt class="field-even">type found_inf<span class="colon">:</span></dt>
<dd class="field-even"><p>Optional[Tensor]</p>
</dd>
<dt class="field-odd">param has_complex<span class="colon">:</span></dt>
<dd class="field-odd"><p>Whether any parameters are complex-valued.</p>
</dd>
<dt class="field-even">type has_complex<span class="colon">:</span></dt>
<dd class="field-even"><p>bool</p>
</dd>
<dt class="field-odd">param beta1<span class="colon">:</span></dt>
<dd class="field-odd"><p>Exponential decay rate for first moment (momentum).
Typical range: 0.9-0.95.</p>
</dd>
<dt class="field-even">type beta1<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
<dt class="field-odd">param beta2<span class="colon">:</span></dt>
<dd class="field-odd"><p>Exponential decay rate for second moment (variance).
Typical range: 0.999-0.9999 (higher than Adam).</p>
</dd>
<dt class="field-even">type beta2<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
<dt class="field-odd">param lr<span class="colon">:</span></dt>
<dd class="field-odd"><p>Learning rate. Can be a scalar Tensor for
dynamic learning rate with capturable=True.</p>
</dd>
<dt class="field-even">type lr<span class="colon">:</span></dt>
<dd class="field-even"><p>Union[float, Tensor]</p>
</dd>
<dt class="field-odd">param clip_lambda<span class="colon">:</span></dt>
<dd class="field-odd"><p>Function that maps step
number to gradient clipping threshold. None disables clipping.</p>
</dd>
<dt class="field-even">type clip_lambda<span class="colon">:</span></dt>
<dd class="field-even"><p>Optional[Callable[[int], float]]</p>
</dd>
<dt class="field-odd">param weight_decay<span class="colon">:</span></dt>
<dd class="field-odd"><p>Weight decay coefficient (L2 penalty).</p>
</dd>
<dt class="field-even">type weight_decay<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
<dt class="field-odd">param decouple<span class="colon">:</span></dt>
<dd class="field-odd"><p>If True, use decoupled weight decay (AdamW-style).
Recommended for better generalization.</p>
</dd>
<dt class="field-even">type decouple<span class="colon">:</span></dt>
<dd class="field-even"><p>bool</p>
</dd>
<dt class="field-odd">param eps<span class="colon">:</span></dt>
<dd class="field-odd"><p>Small constant for numerical stability in normalization.</p>
</dd>
<dt class="field-even">type eps<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
<dt class="field-odd">param maximize<span class="colon">:</span></dt>
<dd class="field-odd"><p>If True, maximize objective instead of minimize.</p>
</dd>
<dt class="field-even">type maximize<span class="colon">:</span></dt>
<dd class="field-even"><p>bool</p>
</dd>
<dt class="field-odd">raises RuntimeError<span class="colon">:</span></dt>
<dd class="field-odd"><p>If torch.jit.script is used with foreach or fused.</p>
</dd>
<dt class="field-even">raises RuntimeError<span class="colon">:</span></dt>
<dd class="field-even"><p>If state_steps contains non-tensor elements.</p>
</dd>
<dt class="field-odd">raises RuntimeError<span class="colon">:</span></dt>
<dd class="field-odd"><p>If fused=True (not yet implemented).</p>
</dd>
<dt class="field-even">raises RuntimeError<span class="colon">:</span></dt>
<dd class="field-even"><p>If lr is Tensor with foreach=True and capturable=False.</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Typically called by ADOPT optimizer, not directly</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">adopt</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">grads</span><span class="o">=</span><span class="p">[</span><span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">exp_avgs</span><span class="o">=</span><span class="p">[</span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">exp_avg_sqs</span><span class="o">=</span><span class="p">[</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">state_steps</span><span class="o">=</span><span class="p">[</span><span class="n">step1</span><span class="p">,</span> <span class="n">step2</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">beta2</span><span class="o">=</span><span class="mf">0.9999</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">clip_lambda</span><span class="o">=</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">s</span><span class="o">**</span><span class="mf">0.25</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">decouple</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">maximize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>For distributed training, this API is compatible with torch/distributed/optim</p></li>
<li><p>The foreach mode is generally preferred for GPU training</p></li>
<li><p>Complex parameters are handled transparently by viewing as real</p></li>
<li><p>First optimization step only initializes variance, doesn’t update parameters</p></li>
</ul>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p>ADOPT class: High-level optimizer interface</p></li>
<li><p>_single_tensor_adopt: Single-tensor implementation details</p></li>
<li><p>_multi_tensor_adopt: Multi-tensor implementation details</p></li>
</ul>
</div>
</section>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../index.html" class="btn btn-neutral float-left" title="dicee.models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../base_model/index.html" class="btn btn-neutral float-right" title="dicee.models.base_model" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Caglar Demir.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>